{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMfrfU7pe3nfRdLib3ewXsB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ruslanmv/Agent-driven-Ontology-Repair-and-Evolution/blob/master/ADORE_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ADORE v1.0\n",
        "\n",
        "**A Design Pattern for Reflective, Agent-Guided Ontology Evolution from LLM-Induced Epistemic Anomalies**"
      ],
      "metadata": {
        "id": "_EqnChiqATez"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In an era where knowledge is constantly evolving, **ontologies** — structured representations of knowledge — play a critical role in AI systems. However, manually updating and maintaining these complex knowledge bases can be a daunting task. This notebook introduces the **Axiom Discovery, Refinement, and Ontology Evolution (ADORE)** program, a cutting-edge, multi-agent framework designed to intelligently automate and streamline this process. Imagine having a dynamic team of AI experts and human overseers working in concert to continuously expand and refine a vast digital encyclopedia. ADORE leverages powerful tools like **Owlready2** for robust ontology management and **LangGraph** for orchestrating the collaborative workflow of its specialized agents. At its core, the system empowers the AI to propose new knowledge while ensuring logical consistency and incorporating essential human oversight, paving the way for more adaptive and accurate knowledge-driven applications.\n",
        "\n",
        "---\n",
        "\n",
        "## Program Workflow: Axiom Discovery and Ontology Evolution (ADORE)\n",
        "\n",
        "The ADORE program operates as a sophisticated, multi-step process designed to intelligently propose, evaluate, and integrate new knowledge (axioms) into an existing **ontology** (a structured representation of knowledge). Imagine it as a team of experts collaborating to refine a medical encyclopedia.\n",
        "\n",
        "Here's a simplified flow:\n",
        "\n",
        "1.  **Environment Setup and Ontology Initialization:** First, the program gets ready by importing all the necessary tools (like **Owlready2** for handling the ontology and **LangGraph** for managing the workflow). It then loads or creates a starting ontology, which in this case, is a basic **Pneumonia ontology** containing initial facts about the disease. This also includes setting up a mock system to use OpenAI's powerful language models, acting as a stand-in for other AI services.\n",
        "\n",
        "2.  **Axiom Proposal (LLM Generator):** An **AI (Large Language Model) agent** steps in, acting as a creative thinker. It analyzes the current ontology and proposes a *new axiom* – a single, formal statement of knowledge – in a specific logical format. For example, it might suggest that \"Pneumonia can be caused by NovelVirusX.\"\n",
        "\n",
        "3.  **Axiom Assessment (Domain Expert & Linguistic Insight Agents):** The proposed axiom then goes through a review process by two specialized agents:\n",
        "    * The **Domain Expert Agent** evaluates the axiom's **medical plausibility** (e.g., \"Does it make sense from a medical perspective?\").\n",
        "    * The **Linguistic Insight Agent** checks its **syntactic correctness** and logical structure (e.g., \"Is it written in the correct logical language?\").\n",
        "\n",
        "4.  **Consistency Check (Consistency Guard Agent):** Next, a crucial step: the **Consistency Guard Agent** takes the proposed axiom and temporarily adds it to a *copy* of the current ontology. It then runs a **reasoner** (a tool that checks for logical contradictions). If the new axiom creates a contradiction with existing knowledge in the ontology, it's flagged as inconsistent.\n",
        "\n",
        "5.  **Human Decision Stage 1 (HITL Stage 1):** A **Human-in-the-Loop (HITL)** agent steps in to make a strategic decision. If the proposed axiom is consistent, the human (simulated here) might choose to \"Accept Axiom\" (though the actual addition happens later). If it's *inconsistent*, the human decides on \"Ontology Evolution,\" indicating that a repair process is needed.\n",
        "\n",
        "6.  **Axiom Weakening/Repair (Axiom Weakening Agent):** If an inconsistency was detected, the **Axiom Weakening Agent** attempts to fix it. This involves modifying or \"weakening\" existing axioms in the ontology or even the proposed axiom itself to resolve the contradiction while trying to retain as much useful information as possible. It proposes a \"repaired\" version of the ontology.\n",
        "\n",
        "7.  **Human Decision Stage 2 (HITL Stage 2):** Another **Human-in-the-Loop** stage occurs. If a repair proposal was made, the human (simulated) reviews it and decides whether to accept the repaired ontology as the new, updated knowledge base.\n",
        "\n",
        "8.  **Knowledge Consolidation & Logging (Meta-Knowledge Agent):** Finally, the **Meta-Knowledge Agent** takes the chosen, consistent version of the ontology (either the one with the accepted axiom or the repaired one) and makes it the *new active ontology*. It also meticulously records all the steps and decisions made during this cycle, creating a detailed log of the knowledge evolution process.\n",
        "\n",
        "This entire cycle is designed to allow the system to continuously learn and expand its knowledge base in a logically sound and human-supervised manner.\n",
        "\n"
      ],
      "metadata": {
        "id": "4F-t7upCDUXd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: Install required packages\n",
        "!pip install owlready2 langgraph python-dotenv langchain-ibm==0.3.10 langchain==0.3.10\n",
        "!pip install langchain_openai\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TxuMUeMZAXZD",
        "outputId": "42efa40f-0984-4a52-b161-cdae99f4942f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting owlready2\n",
            "  Downloading owlready2-0.48.tar.gz (27.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.3/27.3 MB\u001b[0m \u001b[31m78.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting langgraph\n",
            "  Downloading langgraph-0.4.7-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting langchain-ibm==0.3.10\n",
            "  Downloading langchain_ibm-0.3.10-py3-none-any.whl.metadata (5.2 kB)\n",
            "Collecting langchain==0.3.10\n",
            "  Downloading langchain-0.3.10-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting ibm-watsonx-ai<2.0.0,>=1.2.10 (from langchain-ibm==0.3.10)\n",
            "  Downloading ibm_watsonx_ai-1.3.23-py3-none-any.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.39 in /usr/local/lib/python3.11/dist-packages (from langchain-ibm==0.3.10) (0.3.60)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.10) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.10) (2.0.41)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.10) (3.11.15)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.10) (0.3.8)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain==0.3.10)\n",
            "  Downloading langsmith-0.1.147-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting numpy<2,>=1.22.4 (from langchain==0.3.10)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.10) (2.11.4)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.10) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.10) (9.1.2)\n",
            "Collecting langgraph-checkpoint>=2.0.26 (from langgraph)\n",
            "  Downloading langgraph_checkpoint-2.0.26-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting langgraph-prebuilt>=0.2.0 (from langgraph)\n",
            "  Downloading langgraph_prebuilt-0.2.2-py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting langgraph-sdk>=0.1.42 (from langgraph)\n",
            "  Downloading langgraph_sdk-0.1.70-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from langgraph) (3.5.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.10) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.10) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.10) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.10) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.10) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.10) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.10) (1.20.0)\n",
            "Requirement already satisfied: httpx<0.29,>=0.27 in /usr/local/lib/python3.11/dist-packages (from ibm-watsonx-ai<2.0.0,>=1.2.10->langchain-ibm==0.3.10) (0.28.1)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.11/dist-packages (from ibm-watsonx-ai<2.0.0,>=1.2.10->langchain-ibm==0.3.10) (2.4.0)\n",
            "Requirement already satisfied: pandas<2.3.0,>=0.24.2 in /usr/local/lib/python3.11/dist-packages (from ibm-watsonx-ai<2.0.0,>=1.2.10->langchain-ibm==0.3.10) (2.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from ibm-watsonx-ai<2.0.0,>=1.2.10->langchain-ibm==0.3.10) (2025.4.26)\n",
            "Collecting lomond (from ibm-watsonx-ai<2.0.0,>=1.2.10->langchain-ibm==0.3.10)\n",
            "  Downloading lomond-0.3.3-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.11/dist-packages (from ibm-watsonx-ai<2.0.0,>=1.2.10->langchain-ibm==0.3.10) (0.9.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from ibm-watsonx-ai<2.0.0,>=1.2.10->langchain-ibm==0.3.10) (24.2)\n",
            "Collecting ibm-cos-sdk<2.15.0,>=2.12.0 (from ibm-watsonx-ai<2.0.0,>=1.2.10->langchain-ibm==0.3.10)\n",
            "  Downloading ibm_cos_sdk-2.14.1.tar.gz (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.9/58.9 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.39->langchain-ibm==0.3.10) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.39->langchain-ibm==0.3.10) (4.13.2)\n",
            "Collecting ormsgpack<2.0.0,>=1.8.0 (from langgraph-checkpoint>=2.0.26->langgraph)\n",
            "  Downloading ormsgpack-1.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk>=0.1.42->langgraph) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.3.10) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain==0.3.10) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain==0.3.10) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain==0.3.10) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain==0.3.10) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain==0.3.10) (3.10)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.3.10) (3.2.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<0.29,>=0.27->ibm-watsonx-ai<2.0.0,>=1.2.10->langchain-ibm==0.3.10) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<0.29,>=0.27->ibm-watsonx-ai<2.0.0,>=1.2.10->langchain-ibm==0.3.10) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<0.29,>=0.27->ibm-watsonx-ai<2.0.0,>=1.2.10->langchain-ibm==0.3.10) (0.16.0)\n",
            "Collecting ibm-cos-sdk-core==2.14.1 (from ibm-cos-sdk<2.15.0,>=2.12.0->ibm-watsonx-ai<2.0.0,>=1.2.10->langchain-ibm==0.3.10)\n",
            "  Downloading ibm_cos_sdk_core-2.14.1.tar.gz (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m51.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ibm-cos-sdk-s3transfer==2.14.1 (from ibm-cos-sdk<2.15.0,>=2.12.0->ibm-watsonx-ai<2.0.0,>=1.2.10->langchain-ibm==0.3.10)\n",
            "  Downloading ibm_cos_sdk_s3transfer-2.14.1.tar.gz (139 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.6/139.6 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jmespath<=1.0.1,>=0.10.0 (from ibm-cos-sdk<2.15.0,>=2.12.0->ibm-watsonx-ai<2.0.0,>=1.2.10->langchain-ibm==0.3.10)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from ibm-cos-sdk-core==2.14.1->ibm-cos-sdk<2.15.0,>=2.12.0->ibm-watsonx-ai<2.0.0,>=1.2.10->langchain-ibm==0.3.10) (2.9.0.post0)\n",
            "Collecting requests<3,>=2 (from langchain==0.3.10)\n",
            "  Downloading requests-2.32.2-py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.39->langchain-ibm==0.3.10) (3.0.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<2.3.0,>=0.24.2->ibm-watsonx-ai<2.0.0,>=1.2.10->langchain-ibm==0.3.10) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<2.3.0,>=0.24.2->ibm-watsonx-ai<2.0.0,>=1.2.10->langchain-ibm==0.3.10) (2025.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from lomond->ibm-watsonx-ai<2.0.0,>=1.2.10->langchain-ibm==0.3.10) (1.17.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<0.29,>=0.27->ibm-watsonx-ai<2.0.0,>=1.2.10->langchain-ibm==0.3.10) (1.3.1)\n",
            "Downloading langchain_ibm-0.3.10-py3-none-any.whl (28 kB)\n",
            "Downloading langchain-0.3.10-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m47.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph-0.4.7-py3-none-any.whl (154 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.9/154.9 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading ibm_watsonx_ai-1.3.23-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m47.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_checkpoint-2.0.26-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.2/44.2 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_prebuilt-0.2.2-py3-none-any.whl (23 kB)\n",
            "Downloading langgraph_sdk-0.1.70-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.0/50.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langsmith-0.1.147-py3-none-any.whl (311 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.8/311.8 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m95.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.32.2-py3-none-any.whl (63 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.9/63.9 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ormsgpack-1.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (216 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.5/216.5 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lomond-0.3.3-py2.py3-none-any.whl (35 kB)\n",
            "Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Building wheels for collected packages: owlready2, ibm-cos-sdk, ibm-cos-sdk-core, ibm-cos-sdk-s3transfer\n",
            "  Building wheel for owlready2 (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for owlready2: filename=owlready2-0.48-cp311-cp311-linux_x86_64.whl size=24550287 sha256=2d2605a74bed88f8d182fb948f43a079c06de3da017a7e4cc05c4a7127509c0a\n",
            "  Stored in directory: /root/.cache/pip/wheels/2a/4f/b2/88d834aab03077e1611b46825f45c06ac4db07b77ee45eadd5\n",
            "  Building wheel for ibm-cos-sdk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ibm-cos-sdk: filename=ibm_cos_sdk-2.14.1-py3-none-any.whl size=77230 sha256=3be1b1a5ede93d058ffcc14404831243ecdd5c753cfbf58c03ab6ff7815479a8\n",
            "  Stored in directory: /root/.cache/pip/wheels/17/b1/7b/2e3c0f2164c99c9e574fe42e60efdd5701e62743b4aef74f72\n",
            "  Building wheel for ibm-cos-sdk-core (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ibm-cos-sdk-core: filename=ibm_cos_sdk_core-2.14.1-py3-none-any.whl size=662107 sha256=e2769f27187f70c3e90a04ebf8fb10fd15ffd4fab631a99a61aa361f08420319\n",
            "  Stored in directory: /root/.cache/pip/wheels/ff/0a/37/5de8dea48036b23ebd6d0a4709ec572ac69bb901aa5f1019b8\n",
            "  Building wheel for ibm-cos-sdk-s3transfer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ibm-cos-sdk-s3transfer: filename=ibm_cos_sdk_s3transfer-2.14.1-py3-none-any.whl size=90203 sha256=2d4fb6f2f6259f0ca6e68ed3c16ed02c29799fb93a5129047fafaa9ba68f30dc\n",
            "  Stored in directory: /root/.cache/pip/wheels/22/17/80/65e71ce1e1193e7bba209c81704c1a865b9a46edf473303903\n",
            "Successfully built owlready2 ibm-cos-sdk ibm-cos-sdk-core ibm-cos-sdk-s3transfer\n",
            "Installing collected packages: requests, python-dotenv, owlready2, ormsgpack, numpy, lomond, jmespath, ibm-cos-sdk-core, langsmith, langgraph-sdk, ibm-cos-sdk-s3transfer, ibm-cos-sdk, langgraph-checkpoint, ibm-watsonx-ai, langgraph-prebuilt, langchain-ibm, langchain, langgraph\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.3\n",
            "    Uninstalling requests-2.32.3:\n",
            "      Successfully uninstalled requests-2.32.3\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: langsmith\n",
            "    Found existing installation: langsmith 0.3.42\n",
            "    Uninstalling langsmith-0.3.42:\n",
            "      Successfully uninstalled langsmith-0.3.42\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.3.25\n",
            "    Uninstalling langchain-0.3.25:\n",
            "      Successfully uninstalled langchain-0.3.25\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.2 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed ibm-cos-sdk-2.14.1 ibm-cos-sdk-core-2.14.1 ibm-cos-sdk-s3transfer-2.14.1 ibm-watsonx-ai-1.3.23 jmespath-1.0.1 langchain-0.3.10 langchain-ibm-0.3.10 langgraph-0.4.7 langgraph-checkpoint-2.0.26 langgraph-prebuilt-0.2.2 langgraph-sdk-0.1.70 langsmith-0.1.147 lomond-0.3.3 numpy-1.26.4 ormsgpack-1.10.0 owlready2-0.48 python-dotenv-1.1.0 requests-2.32.2\n",
            "Collecting langchain_openai\n",
            "  Downloading langchain_openai-0.3.19-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting langchain-core<1.0.0,>=0.3.63 (from langchain_openai)\n",
            "  Downloading langchain_core-0.3.63-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.68.2 in /usr/local/lib/python3.11/dist-packages (from langchain_openai) (1.81.0)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.11/dist-packages (from langchain_openai) (0.9.0)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.126 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.63->langchain_openai) (0.1.147)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.63->langchain_openai) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.63->langchain_openai) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.63->langchain_openai) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.63->langchain_openai) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.63->langchain_openai) (4.13.2)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.63->langchain_openai) (2.11.4)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain_openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain_openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain_openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain_openai) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain_openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain_openai) (4.67.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2.32.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.68.2->langchain_openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.68.2->langchain_openai) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.68.2->langchain_openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.68.2->langchain_openai) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.63->langchain_openai) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.126->langchain-core<1.0.0,>=0.3.63->langchain_openai) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.126->langchain-core<1.0.0,>=0.3.63->langchain_openai) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.63->langchain_openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.63->langchain_openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.63->langchain_openai) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (2.4.0)\n",
            "Downloading langchain_openai-0.3.19-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.5/64.5 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.3.63-py3-none-any.whl (438 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.5/438.5 kB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: langchain-core, langchain_openai\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.60\n",
            "    Uninstalling langchain-core-0.3.60:\n",
            "      Successfully uninstalled langchain-core-0.3.60\n",
            "Successfully installed langchain-core-0.3.63 langchain_openai-0.3.19\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import base64\n",
        "import requests\n",
        "from IPython.display import Image, display\n",
        "\n",
        "def render_mermaid(mermaid_code):\n",
        "    # Encode the Mermaid code in base64\n",
        "    encoded_code = base64.urlsafe_b64encode(mermaid_code.encode('utf-8')).decode('ascii')\n",
        "    # Construct the URL to fetch the image\n",
        "    url = f\"https://mermaid.ink/img/{encoded_code}\"\n",
        "    # Retrieve and display the image\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        display(Image(response.content))\n",
        "    else:\n",
        "        print(\"Error fetching the Mermaid diagram.\")\n",
        "\n",
        "# Define your Mermaid diagram\n",
        "mermaid_code = \"\"\"\n",
        "graph TD\n",
        "    LLMGen --> DEA\n",
        "    DEA --> LIA\n",
        "    LIA --> CGA\n",
        "    CGA --> HITL1\n",
        "    HITL1 -- Ontology Evolution --> AWEA\n",
        "    HITL1 -- Accept Axiom --> MKA\n",
        "    AWEA --> HITL2\n",
        "    HITL2 --> MKA\n",
        "    MKA --> END\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "kTfQ9ivWESl3"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ADORE Workflow Diagram\n",
        "\n",
        "The following Mermaid diagram provides a visual representation of the ADORE system's flow, illustrating how each agent interacts in this iterative process of knowledge management."
      ],
      "metadata": {
        "id": "tMSWzV5VEi3i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Render the diagram\n",
        "render_mermaid(mermaid_code)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 967
        },
        "id": "SqOmd1J4EgIY",
        "outputId": "e54f8070-8882-4502-8b8f-7062acf86abb"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/jpeg": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "This graph showcases the dynamic interplay between the various components:\n",
        "\n",
        "* **LLMGen (LLM Generator Agent)**: The **starting point** of the workflow, where the AI proposes new axioms to expand the ontology's knowledge.\n",
        "* **DEA (Domain Expert Agent)**: Evaluates the **real-world plausibility** of the proposed axiom, mimicking a subject-matter expert's judgment.\n",
        "* **LIA (Linguistic Insight Agent)**: Assesses the **formal correctness and syntactic coherence** of the axiom in Description Logic.\n",
        "* **CGA (Consistency Guard Agent)**: A **critical checkpoint** that verifies the axiom's compatibility with the existing ontology, flagging any logical inconsistencies.\n",
        "* **HITL1 (Human-in-the-Loop Stage 1)**: The **initial human decision point**, determining whether to proceed with ontology evolution (repair) if inconsistencies are found or to accept the axiom if it's consistent.\n",
        "* **AWEA (Axiom Weakening Agent)**: Engages when inconsistencies are detected, **proposing modifications** to resolve contradictions and restore logical soundness.\n",
        "* **HITL2 (Human-in-the-Loop Stage 2)**: The **secondary human review**, where proposed repairs from AWEA are evaluated and chosen for integration.\n",
        "* **MKA (Meta-Knowledge Agent)**: The **final consolidation stage**, responsible for updating the active ontology with accepted changes and logging the entire cycle for future reference.\n",
        "* **END**: Signifies the **completion of one ADORE cycle**, leaving the ontology updated and ready for continuous refinement.\n",
        "\n",
        "This structured workflow ensures that any new knowledge introduced into the ontology is rigorously vetted for domain relevance, linguistic correctness, and logical consistency, with strategic human checkpoints to guide the automated processes."
      ],
      "metadata": {
        "id": "yRAaRKs7DZ-J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook introduces a comprehensive framework for Axiom Discovery, Refinement, and Ontology Evolution (ADORE), designed to intelligently manage and update knowledge within an OWL ontology. At its core, the system orchestrates various specialized agents using LangGraph, a library for building stateful, multi-actor applications, and leverages Owlready2 for robust ontology manipulation and reasoning. The setup begins by importing necessary libraries and configuring the environment to support both local execution and Google Colab, including secure handling of API keys for large language models. Notably, a MockWatsonxLLM class is implemented to seamlessly integrate OpenAI's ChatOpenAI as a substitute, ensuring the workflow remains functional even without direct access to IBM Watsonx. The initial state of the system is anchored by a foundational \"Pneumonia\" OWL ontology, which is then dynamically updated through a series of interconnected agents: an LLM Generator proposing new axioms, Domain Expert and Linguistic Insight Agents assessing their quality, a Consistency Guard Agent validating their logical coherence, and a Human-in-the-Loop stage for strategic decision-making. Should inconsistencies arise, an Axiom Weakening Agent proposes repairs, leading to a second Human-in-the-Loop review. Finally, a Meta-Knowledge Agent consolidates the process by logging the cycle's outcomes and updating the ontology for subsequent iterations, demonstrating a sophisticated approach to automated and human-guided ontology evolution."
      ],
      "metadata": {
        "id": "PJT4xtwpBF1l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: Imports and Environment Setup\n",
        "\n",
        "import os\n",
        "import copy\n",
        "import json\n",
        "import tempfile  # Needed for ontology duplication\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "import owlready2\n",
        "\n",
        "# Owlready2 for ontology manipulation & reasoning\n",
        "from owlready2 import get_ontology, Thing, ObjectProperty, DataProperty, sync_reasoner, World # Added DataProperty\n",
        "\n",
        "# Attempt to import restriction and construct classes from their typical module\n",
        "try:\n",
        "    from owlready2.class_construct import (\n",
        "        Restriction, SomeValuesFrom, AllValuesFrom,\n",
        "        MinCardinality, MaxCardinality, ExactCardinality,\n",
        "        HasValue # Add others if used by your logic\n",
        "    )\n",
        "    CLASS_CONSTRUCT_IMPORTED = True\n",
        "    print(\"Successfully imported Restriction classes from owlready2.class_construct.\")\n",
        "except ImportError:\n",
        "    CLASS_CONSTRUCT_IMPORTED = False\n",
        "    print(\"Warning: Could not import class_construct from Owlready2. \"\n",
        "          \"Axiom processing involving SomeValuesFrom, AllValuesFrom, etc., might fail.\")\n",
        "    # Define fallbacks so the script doesn't immediately crash if these names are used later\n",
        "    # and the import failed. This allows the error to occur where they are used.\n",
        "    SomeValuesFrom, AllValuesFrom = None, None\n",
        "    MinCardinality, MaxCardinality, ExactCardinality = None, None, None\n",
        "    Restriction = None # Define Restriction as well if it's used as a base type check\n",
        "\n",
        "# LangGraph for orchestration\n",
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "# LangChain schema for messages\n",
        "from langchain.schema import HumanMessage, AIMessage\n",
        "\n",
        "# Conditional imports for Colab\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "    from google.colab import userdata\n",
        "\n",
        "# New import for OpenAI\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Load .env (if running outside Colab)\n",
        "if not IN_COLAB:  # Only load dotenv if not in Colab\n",
        "    load_dotenv()\n",
        "\n",
        "# Fetch IBM Watsonx credentials (will be ignored by mock but kept for signature)\n",
        "if IN_COLAB:\n",
        "    API_KEY_WATSONX = userdata.get(\"WATSONX_API_KEY\")\n",
        "    PROJECT_ID_WATSONX = userdata.get(\"PROJECT_ID\")\n",
        "    URL_WATSONX = userdata.get(\"WATSONX_URL\")\n",
        "else:\n",
        "    API_KEY_WATSONX = os.getenv(\"WATSONX_API_KEY\")\n",
        "    PROJECT_ID_WATSONX = os.getenv(\"PROJECT_ID\")\n",
        "    URL_WATSONX = os.getenv(\"WATSONX_URL\")\n",
        "\n",
        "# Fallback check, though these specific keys won't be used by the OpenAI mock\n",
        "if not API_KEY_WATSONX or not PROJECT_ID_WATSONX or not URL_WATSONX:\n",
        "    print(\"Warning: WATSONX_API_KEY, PROJECT_ID, or WATSONX_URL not set. Mock will proceed with OpenAI.\")\n",
        "    # Not raising RuntimeError to allow OpenAI mock to function without these.\n",
        "\n",
        "# Fetch OpenAI personal API key\n",
        "if IN_COLAB:\n",
        "    OPENAI_API_KEY = userdata.get(\"OPENAI_API_KEY_PERSONAL\")\n",
        "else:\n",
        "    OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY_PERSONAL\")\n",
        "\n",
        "if not OPENAI_API_KEY:\n",
        "    raise RuntimeError(\"Please set OPENAI_API_KEY_PERSONAL in environment or Colab userdata.\")\n",
        "\n",
        "# Define the OpenAI model ID to be used\n",
        "OPENAI_MODEL_ID = \"gpt-3.5-turbo\"\n",
        "\n",
        "\n",
        "# Cell 2: Mocking WatsonxLLM with OpenAI Under the Hood\n",
        "\n",
        "class MockWatsonxLLM:\n",
        "    def __init__(self, model_id, url, apikey, project_id, params):\n",
        "        self.model_id = model_id  # Watsonx model_id (unused by OpenAI)\n",
        "        self.url = url            # Watsonx URL (unused by OpenAI)\n",
        "        self.apikey = apikey      # Watsonx API key (unused by OpenAI)\n",
        "        self.project_id = project_id  # Watsonx Project ID (unused by OpenAI)\n",
        "        self.params = params\n",
        "\n",
        "        self.openai_llm = ChatOpenAI(\n",
        "            model=OPENAI_MODEL_ID,\n",
        "            openai_api_key=OPENAI_API_KEY,  # Actual OpenAI key\n",
        "            temperature=self.params.get(\"temperature\", 0.7),\n",
        "            max_tokens=self.params.get(\"max_new_tokens\", 200)\n",
        "        )\n",
        "        print(\"Initialized MockWatsonxLLM (secretly using OpenAI)\")\n",
        "\n",
        "    def invoke(self, messages):\n",
        "        print(f\"MockWatsonxLLM received messages (forwarding to OpenAI): {messages}\")\n",
        "        try:\n",
        "            openai_response_obj = self.openai_llm.invoke(messages)\n",
        "            return openai_response_obj\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred during OpenAI LLM invocation within mock: {e}\")\n",
        "            return AIMessage(content=\"This is a fallback mock response due to an OpenAI error.\")\n",
        "\n",
        "watsonx_llm = MockWatsonxLLM(\n",
        "    model_id=\"some_watsonx_model_id\",          # Placeholder\n",
        "    url=URL_WATSONX or \"http://fake.watsonx.url\",  # Placeholder or loaded\n",
        "    apikey=API_KEY_WATSONX or \"fake_watsonx_apikey\",  # Placeholder or loaded\n",
        "    project_id=PROJECT_ID_WATSONX or \"fake_watsonx_project_id\",  # Placeholder or loaded\n",
        "    params={\n",
        "        \"decoding_method\": \"greedy\",\n",
        "        \"max_new_tokens\": 200,\n",
        "        \"temperature\": 0.7\n",
        "    }\n",
        ")\n",
        "print(\"Using watsonx_llm instance:\", isinstance(watsonx_llm, MockWatsonxLLM))\n",
        "\n",
        "\n",
        "# Cell 3: Build the Initial Pneumonia Ontology in Owlready2\n",
        "\n",
        "ONTOLOGY_IRI = \"http://example.org/pneumonia_ontology.owl\"\n",
        "onto = get_ontology(ONTOLOGY_IRI)  # Operates in default_world\n",
        "temp_onto_file_path = None # Define to ensure it's available in global scope for Cell 6\n",
        "\n",
        "with onto:\n",
        "    class Pneumonia(Thing):\n",
        "        pass\n",
        "    class Bacterium(Thing):\n",
        "        pass\n",
        "    class NovelVirusX(Thing):\n",
        "        pass\n",
        "    class Pathogen(Thing):\n",
        "        pass\n",
        "\n",
        "    class causedBy(ObjectProperty):\n",
        "        domain = [Pneumonia]\n",
        "        range = [Thing]\n",
        "\n",
        "    # Ensure SomeValuesFrom and MaxCardinality are available for ontology construction\n",
        "    # Owlready2's .some() and .max() are Pythonic ways to create restrictions.\n",
        "    # They should internally use the correct classes if Owlready2 is functioning properly.\n",
        "    # The CLASS_CONSTRUCT_IMPORTED check is more for isinstance checks later.\n",
        "    try:\n",
        "        Pneumonia.is_a.append(causedBy.some(Bacterium))\n",
        "        Bacterium.disjoint_with = [NovelVirusX]\n",
        "        Pneumonia.is_a.append(causedBy.max(1, Thing))\n",
        "    except Exception as e_construct:\n",
        "        print(f\"Error during initial axiom construction (e.g. .some, .max): {e_construct}\")\n",
        "        print(\"This might happen if Owlready2's internal restriction classes are not found due to import issues.\")\n",
        "\n",
        "\n",
        "# Save and run reasoner\n",
        "try:\n",
        "    # Create a temporary file for saving to avoid issues with existing files if any\n",
        "    temp_onto_file = tempfile.NamedTemporaryFile(delete=False, suffix=\".owl\")\n",
        "    temp_onto_file_path = temp_onto_file.name # Assign to the globally defined variable\n",
        "    temp_onto_file.close()\n",
        "    onto.save(file=temp_onto_file_path, format=\"rdfxml\")\n",
        "    print(f\"Initial ontology saved to temporary file: {temp_onto_file_path}\")\n",
        "\n",
        "    with onto: # Use the main ontology's world\n",
        "        sync_reasoner()\n",
        "    print(\"Initial ontology created, saved, and checked for consistency.\")\n",
        "    # Clean up the temporary file after use, it's not needed anymore for 'onto'\n",
        "    # The path is stored in initial_state if needed for later reference/cleanup.\n",
        "    # However, _duplicate_ontology will use this file if it's the most recent save.\n",
        "    # For this script's flow, it might be better to keep it until the state is copied or the script ends.\n",
        "    # For now, let's not delete it here, MKA and final cleanup can handle it.\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error during initial ontology setup: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "    # Depending on severity, you might want to raise e or handle it\n",
        "\n",
        "\n",
        "# Cell 4: Helper Function to Duplicate Ontology and Inspect\n",
        "\n",
        "def _duplicate_ontology(source_ontology):\n",
        "    \"\"\"\n",
        "    Creates an isolated duplicate of an Owlready2 ontology.\n",
        "    Returns the new ontology instance and the path to its temporary file.\n",
        "    \"\"\"\n",
        "    if source_ontology is None:\n",
        "        raise ValueError(\"Cannot duplicate a None ontology.\")\n",
        "\n",
        "    # Create a new world for the duplicate to ensure isolation\n",
        "    isolated_world = World()\n",
        "\n",
        "    # Save the source ontology to a temporary file\n",
        "    # Each duplication gets its own fresh temp file.\n",
        "    temp_file_handler_dup = tempfile.NamedTemporaryFile(delete=False, suffix=\".owl\")\n",
        "    temp_file_path_dup = temp_file_handler_dup.name\n",
        "    temp_file_handler_dup.close()\n",
        "\n",
        "    source_ontology.save(file=temp_file_path_dup, format=\"rdfxml\")\n",
        "\n",
        "    file_iri = f\"file://{temp_file_path_dup}\"\n",
        "    # Load the ontology into the new, isolated world\n",
        "    copied_onto = isolated_world.get_ontology(file_iri).load()\n",
        "\n",
        "    print(f\"Duplicated ontology {source_ontology.name} into new world with IRI {copied_onto.base_iri} (temp file: {temp_file_path_dup})\")\n",
        "    return copied_onto, temp_file_path_dup  # Return path for potential cleanup\n",
        "\n",
        "\n",
        "def is_consistent(ontology_to_check):\n",
        "    \"\"\"\n",
        "    Returns True if the ontology is consistent. Uses a duplicated ontology for the check.\n",
        "    \"\"\"\n",
        "    if ontology_to_check is None:\n",
        "        print(\"is_consistent: Received None ontology, returning False.\")\n",
        "        return False\n",
        "\n",
        "    temp_onto_for_consistency = None\n",
        "    temp_file_path_for_consistency = None\n",
        "    try:\n",
        "        temp_onto_for_consistency, temp_file_path_for_consistency = _duplicate_ontology(ontology_to_check)\n",
        "        with temp_onto_for_consistency:\n",
        "            sync_reasoner(infer_property_values=False)\n",
        "        # If reasoner runs without error, assume consistent for this check's purpose\n",
        "        # A more robust check would be `not list(temp_onto_for_consistency.inconsistent_classes())`\n",
        "        # but that requires HermiT to populate it, which it does.\n",
        "        return not list(temp_onto_for_consistency.world.inconsistent_classes())\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"is_consistent: Consistency check failed or error during duplication/reasoning: {e}\")\n",
        "        return False\n",
        "    finally:\n",
        "        if temp_onto_for_consistency and hasattr(temp_onto_for_consistency, '_world'):\n",
        "             if hasattr(temp_onto_for_consistency._world, 'closed') and not temp_onto_for_consistency._world.closed:\n",
        "                if temp_onto_for_consistency._world is not owlready2.default_world: # Don't close default_world\n",
        "                    temp_onto_for_consistency._world.close()\n",
        "                    print(f\"is_consistent: Closed isolated world for {temp_onto_for_consistency.name}\")\n",
        "        if temp_file_path_for_consistency and os.path.exists(temp_file_path_for_consistency):\n",
        "            try:\n",
        "                os.unlink(temp_file_path_for_consistency)\n",
        "            except Exception as e_unlink:\n",
        "                print(f\"is_consistent: Error unlinking temp file {temp_file_path_for_consistency}: {e_unlink}\")\n",
        "\n",
        "\n",
        "def print_axioms(ontology_to_print):\n",
        "    \"\"\"\n",
        "    Print all logical axioms (TBox) in a readable form.\n",
        "    \"\"\"\n",
        "    if ontology_to_print is None:\n",
        "        print(\"\\n=== print_axioms: Ontology is None. Cannot print axioms. ===\")\n",
        "        return\n",
        "\n",
        "    print(f\"\\n=== Current Ontology Axioms ({ontology_to_print.name}) ===\")\n",
        "    try:\n",
        "        for cls in ontology_to_print.classes():\n",
        "            for sup in cls.is_a:\n",
        "                print(f\"{cls.name} ⊑ {sup}\")\n",
        "        for dis_axiom_obj in ontology_to_print.disjoint_classes():\n",
        "            if hasattr(dis_axiom_obj, 'entities'):\n",
        "                entity_names = [e.name for e in dis_axiom_obj.entities if hasattr(e, 'name')]\n",
        "                if entity_names:\n",
        "                    print(f\"DisjointClasses({', '.join(entity_names)})\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error printing axioms: {e}\")\n",
        "    print(\"=== End of Axioms ===\\n\")\n",
        "\n",
        "\n",
        "# Cell 5: Define Agents as Python Functions\n",
        "\n",
        "# 5.1. LLM Generator Agent\n",
        "def llm_generator(state):\n",
        "    print(\"--- Running LLM Generator Agent ---\")\n",
        "    prompt = (\n",
        "        \"Given the ontology axioms:\\n\"\n",
        "        \"1) Pneumonia ⊑ ∃causedBy.Bacterium\\n\"\n",
        "        \"2) NovelVirusX ⊑ ¬Bacterium\\n\"\n",
        "        \"3) Pneumonia ⊑ ≤1 causedBy.⊤\\n\"\n",
        "        \"\\n\"\n",
        "        \"Please propose a single new DL axiom suggesting that Pneumonia \"\n",
        "        \"can be caused by NovelVirusX, written in Description Logic syntax. \"\n",
        "        \"Output exactly one line like:\\n\"\n",
        "        \"Pneumonia ⊑ ∃causedBy.NovelVirusX\\n\"\n",
        "    )\n",
        "    print(\"Send Prompt to AI:\",prompt)\n",
        "    response_obj = watsonx_llm.invoke([HumanMessage(content=prompt)])\n",
        "    if response_obj and hasattr(response_obj, 'content') and response_obj.content:\n",
        "        response_string = response_obj.content\n",
        "        axiom_str = response_string.strip().splitlines()[0]\n",
        "        print(\"AI proposed axiom:\",axiom_str)\n",
        "        log_action = \"proposed_axiom\"\n",
        "    else:\n",
        "        axiom_str = \"Pneumonia ⊑ ∃causedBy.NovelVirusX\"\n",
        "        log_action = \"proposed_axiom_fallback_llm_error\"\n",
        "        print(f\"LLMGen: Warning - LLM response was empty or invalid. Using fallback axiom: {axiom_str}\")\n",
        "\n",
        "    state[\"candidate_axiom\"] = axiom_str\n",
        "    state[\"log\"].append({\"agent\": \"LLMGen\", \"action\": log_action, \"axiom\": axiom_str})\n",
        "    return state\n",
        "\n",
        "\n",
        "# 5.2. Domain Expert Agent (DEA)\n",
        "def domain_expert_agent(state):\n",
        "    print(\"--- Running Domain Expert Agent ---\")\n",
        "    ax = state.get(\"candidate_axiom\")\n",
        "    score = 0.9 if isinstance(ax, str) and \"NovelVirusX\" in ax else 0.1\n",
        "    justification = (\"Contains NovelVirusX, which is medically plausible.\" if score > 0.5\n",
        "                     else \"Unclear entity or does not seem plausible.\")\n",
        "    state[\"dea_assessment\"] = {\"score\": score, \"justification\": justification}\n",
        "    state[\"log\"].append({\"agent\": \"DEA\", \"assessment\": state[\"dea_assessment\"]})\n",
        "    return state\n",
        "\n",
        "\n",
        "# 5.3. Linguistic Insight Agent (LIA)\n",
        "def linguistic_insight_agent(state):\n",
        "    print(\"--- Running Linguistic Insight Agent ---\")\n",
        "    ax = state.get(\"candidate_axiom\")\n",
        "    if isinstance(ax, str) and \"⊑ ∃\" in ax and \".\" in ax:\n",
        "        score, justification = 0.8, \"Syntactically coherent DL axiom.\"\n",
        "    else:\n",
        "        score, justification = 0.2, \"Does not match expected DL pattern or not a string.\"\n",
        "    state[\"lia_assessment\"] = {\"score\": score, \"justification\": justification}\n",
        "    state[\"log\"].append({\"agent\": \"LIA\", \"assessment\": state[\"lia_assessment\"]})\n",
        "    return state\n",
        "\n",
        "\n",
        "# 5.4. Consistency Guard Agent (CGA)\n",
        "def consistency_guard_agent(state):\n",
        "    print(\"--- Running Consistency Guard Agent ---\")\n",
        "    orig_onto = state.get(\"ontology\")\n",
        "    candidate_ax_str = state.get(\"candidate_axiom\")\n",
        "    temp_file_path_cga = None\n",
        "    temp_onto_cga = None\n",
        "\n",
        "    if orig_onto is None:\n",
        "        state[\"consistent_with_candidate\"] = False\n",
        "        state[\"mis_list\"] = [\"<main_ontology_not_found_in_state_for_CGA>\"]\n",
        "        state[\"log\"].append({\"agent\": \"CGA\", \"status\": \"ontology_error\", \"error\": \"Main ontology (orig_onto) is None.\"})\n",
        "        return state\n",
        "\n",
        "    if not isinstance(candidate_ax_str, str):\n",
        "        state[\"consistent_with_candidate\"] = False\n",
        "        state[\"mis_list\"] = [f\"<invalid_candidate_axiom_format_not_string: {type(candidate_ax_str)}>\"]\n",
        "        state[\"log\"].append({\"agent\": \"CGA\", \"status\": \"parsing_error\", \"error\": f\"Candidate axiom not a string: {type(candidate_ax_str)}\"})\n",
        "        return state\n",
        "\n",
        "    try:\n",
        "        temp_onto_cga, temp_file_path_cga = _duplicate_ontology(orig_onto)\n",
        "        print(f\"CGA: Successfully duplicated ontology. Original: {orig_onto.name}, Temp copy: {temp_onto_cga.name}\")\n",
        "\n",
        "        lhs_str, rhs_str = [s.strip() for s in candidate_ax_str.split(\"⊑\")]\n",
        "\n",
        "        if rhs_str.startswith(\"∃\") and \".\" in rhs_str:\n",
        "            prop_filler_part = rhs_str[1:]\n",
        "            prop_name, filler_name = prop_filler_part.split(\".\", 1)\n",
        "\n",
        "            lhs_class_temp = getattr(temp_onto_cga, lhs_str, None)\n",
        "            prop_temp = getattr(temp_onto_cga, prop_name, None)\n",
        "            filler_class_temp = getattr(temp_onto_cga, filler_name, None)\n",
        "\n",
        "            missing_entities = []\n",
        "            if not lhs_class_temp: missing_entities.append(f\"LHS class '{lhs_str}'\")\n",
        "            if not prop_temp: missing_entities.append(f\"Property '{prop_name}'\")\n",
        "            if not filler_class_temp: missing_entities.append(f\"Filler class '{filler_name}'\")\n",
        "\n",
        "            if missing_entities:\n",
        "                error_msg = f\"CGA: Could not find all required entities in temp_onto_cga ({temp_onto_cga.name}): {', '.join(missing_entities)}.\"\n",
        "                raise ValueError(error_msg)\n",
        "\n",
        "            if not issubclass(prop_temp, ObjectProperty): # CORRECTED CHECK\n",
        "                error_msg = (f\"CGA: Entity '{prop_name}' in temp_onto_cga (value: {prop_temp}, type: {type(prop_temp)}) \"\n",
        "                             f\"is not an ObjectProperty. Expected a subclass of owlready2.ObjectProperty.\")\n",
        "                raise ValueError(error_msg)\n",
        "\n",
        "            # Ensure SomeValuesFrom is available for creating the axiom if needed by Owlready2 internals\n",
        "            # The .some() method should handle this, but good to be aware if CLASS_CONSTRUCT_IMPORTED is False\n",
        "            if not (CLASS_CONSTRUCT_IMPORTED and SomeValuesFrom):\n",
        "                 print(\"CGA: Warning - SomeValuesFrom class not imported, relying on property.some() internal behavior.\")\n",
        "\n",
        "\n",
        "            new_axiom = prop_temp.some(filler_class_temp)\n",
        "            lhs_class_temp.is_a.append(new_axiom)\n",
        "            print(f\"CGA: Added to temp_onto_cga '{temp_onto_cga.name}': {lhs_class_temp.name} ⊑ {prop_temp.name} some {filler_class_temp.name}\")\n",
        "        else:\n",
        "            raise ValueError(f\"CGA: Unsupported axiom structure for parsing: {candidate_ax_str}\")\n",
        "\n",
        "        with temp_onto_cga:\n",
        "            sync_reasoner(infer_property_values=False)\n",
        "\n",
        "        state[\"consistent_with_candidate\"] = True\n",
        "        state[\"mis_list\"] = []\n",
        "        state[\"log\"].append({\"agent\": \"CGA\", \"status\": \"consistent\", \"axiom_added_to_temp\": candidate_ax_str})\n",
        "        print(f\"CGA: Candidate axiom '{candidate_ax_str}' is consistent with the (temporarily modified) ontology.\")\n",
        "\n",
        "    except ValueError as ve:\n",
        "        print(f\"CGA: Axiom parsing/application error: {ve}\")\n",
        "        state[\"consistent_with_candidate\"] = False\n",
        "        state[\"mis_list\"] = [f\"<parsing_value_error_cga: {str(ve)}>\", candidate_ax_str]\n",
        "        state[\"log\"].append({\"agent\": \"CGA\", \"status\": \"parsing_error\", \"error\": str(ve)})\n",
        "    except Exception as e:\n",
        "        import traceback\n",
        "        print(f\"CGA: Inconsistency detected or other error during reasoning/check: {e}\")\n",
        "        print(traceback.format_exc())\n",
        "        state[\"consistent_with_candidate\"] = False\n",
        "        state[\"mis_list\"] = [\n",
        "            \"Pneumonia ⊑ ∃causedBy.Bacterium\", \"NovelVirusX ⊑ ¬Bacterium\",\n",
        "            \"Pneumonia ⊑ ≤1 causedBy.⊤\", candidate_ax_str\n",
        "        ]\n",
        "        state[\"log\"].append({\n",
        "            \"agent\": \"CGA\", \"status\": \"inconsistent_or_error_cga\",\n",
        "            \"mis_simulation\": state[\"mis_list\"], \"error_details\": str(e)\n",
        "        })\n",
        "    finally:\n",
        "        if temp_onto_cga and hasattr(temp_onto_cga, '_world') and temp_onto_cga._world is not orig_onto._world:\n",
        "             if hasattr(temp_onto_cga._world, 'closed') and not temp_onto_cga._world.closed:\n",
        "                if temp_onto_cga._world is not owlready2.default_world:\n",
        "                    temp_onto_cga._world.close()\n",
        "                    print(f\"CGA: Closed isolated world for {temp_onto_cga.name}\")\n",
        "        if temp_file_path_cga and os.path.exists(temp_file_path_cga):\n",
        "            try:\n",
        "                os.unlink(temp_file_path_cga)\n",
        "            except Exception as unlink_e:\n",
        "                print(f\"CGA: Error unlinking temp file {temp_file_path_cga}: {unlink_e}\")\n",
        "    return state\n",
        "\n",
        "\n",
        "# 5.5. Human-in-the-Loop (Stage 1)\n",
        "def hitl_stage1(state):\n",
        "    print(\"--- Running HITL Stage 1 ---\")\n",
        "    if not state.get(\"consistent_with_candidate\", True):\n",
        "        state[\"chosen_strategy\"] = \"Ontology Evolution\"\n",
        "        rationale = \"Inconsistency detected, choosing ontology evolution to repair.\"\n",
        "    else:\n",
        "        state[\"chosen_strategy\"] = \"Accept Axiom\"\n",
        "        rationale = (\"Candidate axiom is consistent. \"\n",
        "                     \"Simulating choice to accept (no direct addition in this step).\")\n",
        "    state[\"log\"].append({\n",
        "        \"agent\": \"HITL1\",\n",
        "        \"strategy\": state[\"chosen_strategy\"],\n",
        "        \"rationale\": rationale\n",
        "    })\n",
        "    return state\n",
        "\n",
        "\n",
        "# 5.6. Axiom Weakening Agent (AWEA)\n",
        "def axiom_weakening_agent(state):\n",
        "    print(\"--- Running Axiom Weakening Agent ---\")\n",
        "    if state.get(\"chosen_strategy\") != \"Ontology Evolution\" or state.get(\"consistent_with_candidate\", True):\n",
        "        state[\"repair_proposals\"] = []\n",
        "        state[\"log\"].append({\"agent\": \"AWEA\", \"status\": \"skipped_or_no_repair_needed\", \"reason\": state.get(\"chosen_strategy\")})\n",
        "        return state\n",
        "\n",
        "    orig_onto = state.get(\"ontology\")\n",
        "    candidate_ax_str = state.get(\"candidate_axiom\")\n",
        "    temp_file_path_awea = None\n",
        "    repaired_onto = None\n",
        "\n",
        "    if orig_onto is None:\n",
        "        state[\"repair_proposals\"] = []\n",
        "        state[\"log\"].append({\"agent\": \"AWEA\", \"status\": \"ontology_error\", \"error\": \"Main ontology (orig_onto) is None.\"})\n",
        "        return state\n",
        "\n",
        "    try:\n",
        "        repaired_onto, temp_file_path_awea = _duplicate_ontology(orig_onto)\n",
        "        print(f\"AWEA: Successfully duplicated ontology for repair. Original: {orig_onto.name}, Repaired copy: {repaired_onto.name}\")\n",
        "\n",
        "        Pneumonia_cls_rep = getattr(repaired_onto, \"Pneumonia\", None)\n",
        "        Bacterium_cls_rep = getattr(repaired_onto, \"Bacterium\", None)\n",
        "        NovelVirusX_cls_rep = getattr(repaired_onto, \"NovelVirusX\", None)\n",
        "        Pathogen_cls_rep = getattr(repaired_onto, \"Pathogen\", None)\n",
        "        causedBy_prop_rep = getattr(repaired_onto, \"causedBy\", None)\n",
        "\n",
        "        required_entities = {\"Pneumonia\": Pneumonia_cls_rep, \"Bacterium\": Bacterium_cls_rep,\n",
        "                             \"NovelVirusX\": NovelVirusX_cls_rep, \"Pathogen\": Pathogen_cls_rep,\n",
        "                             \"causedBy\": causedBy_prop_rep}\n",
        "        missing = [name for name, entity in required_entities.items() if entity is None]\n",
        "        if missing:\n",
        "            raise ValueError(f\"AWEA: Could not find required entities in repaired_onto ({repaired_onto.name}): {', '.join(missing)}\")\n",
        "\n",
        "        if not issubclass(causedBy_prop_rep, ObjectProperty): # CORRECTED CHECK\n",
        "            raise ValueError(\n",
        "                f\"AWEA: 'causedBy' in repaired_onto ({causedBy_prop_rep}, type: {type(causedBy_prop_rep)}) \"\n",
        "                f\"is not an ObjectProperty. Expected a subclass of owlready2.ObjectProperty.\"\n",
        "            )\n",
        "\n",
        "        ax1_to_remove = None\n",
        "        removed_ax1_flag = False\n",
        "        if CLASS_CONSTRUCT_IMPORTED and SomeValuesFrom: # GUARDED\n",
        "            for axiom in list(Pneumonia_cls_rep.is_a):\n",
        "                if (isinstance(axiom, SomeValuesFrom) and hasattr(axiom, 'property') and # GUARDED USE\n",
        "                        axiom.property == causedBy_prop_rep and hasattr(axiom, 'filler') and\n",
        "                        axiom.filler == Bacterium_cls_rep):\n",
        "                    ax1_to_remove = axiom\n",
        "                    break\n",
        "            if ax1_to_remove:\n",
        "                Pneumonia_cls_rep.is_a.remove(ax1_to_remove)\n",
        "                removed_ax1_flag = True\n",
        "                print(f\"AWEA: Removed 'Pneumonia ⊑ ∃causedBy.Bacterium' from {repaired_onto.name}.\")\n",
        "            else:\n",
        "                print(f\"AWEA: Ax1 'Pneumonia ⊑ ∃causedBy.Bacterium' not found in {Pneumonia_cls_rep.name}.is_a list for removal.\")\n",
        "        else:\n",
        "            print(\"AWEA: Warning - SomeValuesFrom class not imported. Cannot reliably check/remove specific existential restriction axiom.\")\n",
        "\n",
        "        Pneumonia_cls_rep.is_a.append(causedBy_prop_rep.some(Pathogen_cls_rep))\n",
        "        print(f\"AWEA: Added 'Pneumonia ⊑ ∃causedBy.Pathogen' to {repaired_onto.name}.\")\n",
        "\n",
        "        Bacterium_cls_rep.is_a.append(Pathogen_cls_rep)\n",
        "        NovelVirusX_cls_rep.is_a.append(Pathogen_cls_rep)\n",
        "        print(f\"AWEA: Added bridging axioms for Bacterium and NovelVirusX to Pathogen in {repaired_onto.name}.\")\n",
        "\n",
        "        if isinstance(candidate_ax_str, str) and \"⊑ ∃\" in candidate_ax_str:\n",
        "            try:\n",
        "                lhs_cand_str, rhs_cand_str = [s.strip() for s in candidate_ax_str.split(\"⊑\")]\n",
        "                if rhs_cand_str.startswith(\"∃\") and \".\" in rhs_cand_str:\n",
        "                    prop_cand_filler_part = rhs_cand_str[1:]\n",
        "                    prop_cand_name, filler_cand_name = prop_cand_filler_part.split(\".\", 1)\n",
        "\n",
        "                    lhs_cand_cls_rep = getattr(repaired_onto, lhs_cand_str, None)\n",
        "                    prop_cand_rep = getattr(repaired_onto, prop_cand_name, None)\n",
        "                    filler_cand_cls_rep = getattr(repaired_onto, filler_cand_name, None)\n",
        "\n",
        "                    if (all([lhs_cand_cls_rep, prop_cand_rep, filler_cand_cls_rep]) and\n",
        "                            issubclass(prop_cand_rep, ObjectProperty)): # CORRECTED CHECK\n",
        "                        lhs_cand_cls_rep.is_a.append(prop_cand_rep.some(filler_cand_cls_rep))\n",
        "                        print(f\"AWEA: Added candidate axiom '{candidate_ax_str}' to {repaired_onto.name}.\")\n",
        "                    else:\n",
        "                        print(f\"AWEA: Warning - Could not fully parse/apply candidate axiom '{candidate_ax_str}' during repair. \"\n",
        "                              f\"Entities missing or property type incorrect in {repaired_onto.name}.\")\n",
        "                else:\n",
        "                    print(f\"AWEA: Warning - Candidate axiom '{candidate_ax_str}' format not fully supported for re-adding during repair.\")\n",
        "            except Exception as parse_err:\n",
        "                print(f\"AWEA: Warning - Error parsing/adding candidate axiom '{candidate_ax_str}' during repair: {parse_err}\")\n",
        "        else:\n",
        "            print(f\"AWEA: Warning - Candidate axiom '{candidate_ax_str}' is not a string or invalid format for repair addition.\")\n",
        "\n",
        "        with repaired_onto:\n",
        "            sync_reasoner(infer_property_values=False)\n",
        "\n",
        "        repair_justification_text = (\n",
        "            f\"Weakened Ax1 (removed: {removed_ax1_flag}) to use Pathogen, added bridging axioms, \"\n",
        "            f\"and included the candidate axiom '{candidate_ax_str}'.\"\n",
        "        )\n",
        "        state[\"repair_proposals\"] = [{\"ontology\": repaired_onto, \"justification\": repair_justification_text, \"temp_file_path\": temp_file_path_awea}]\n",
        "        state[\"log\"].append({\"agent\": \"AWEA\", \"status\": \"proposed_repair\", \"justification\": repair_justification_text})\n",
        "        print(f\"AWEA: Repaired ontology ({repaired_onto.name}) is consistent and proposed.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        import traceback\n",
        "        print(f\"AWEA: Repair process failed or repaired ontology is inconsistent: {e}\")\n",
        "        print(traceback.format_exc())\n",
        "        state[\"repair_proposals\"] = []\n",
        "        state[\"log\"].append({\"agent\": \"AWEA\", \"status\": \"no_valid_repair_due_to_error_awea\", \"error\": str(e)})\n",
        "    finally:\n",
        "        if repaired_onto and hasattr(repaired_onto, '_world') and repaired_onto._world is not orig_onto._world:\n",
        "            if hasattr(repaired_onto._world, 'closed') and not repaired_onto._world.closed:\n",
        "                if repaired_onto._world is not owlready2.default_world:\n",
        "                    repaired_onto._world.close()\n",
        "                    print(f\"AWEA: Closed isolated world for {repaired_onto.name}\")\n",
        "        if temp_file_path_awea and os.path.exists(temp_file_path_awea):\n",
        "            try:\n",
        "                os.unlink(temp_file_path_awea)\n",
        "            except Exception as unlink_e:\n",
        "                print(f\"AWEA: Error unlinking temp file {temp_file_path_awea} after failure: {unlink_e}\")\n",
        "    return state\n",
        "\n",
        "\n",
        "# 5.7. Human-in-the-Loop (Stage 2)\n",
        "def hitl_stage2(state):\n",
        "    print(\"--- Running HITL Stage 2 ---\")\n",
        "    repair_proposals = state.get(\"repair_proposals\", [])\n",
        "\n",
        "    if not repair_proposals:\n",
        "        state[\"final_choice_ontology\"] = None\n",
        "        state[\"final_choice_temp_file_path\"] = None\n",
        "        state[\"log\"].append({\"agent\": \"HITL2\", \"decision\": \"no_valid_proposals_to_accept\"})\n",
        "        print(\"HITL2: No repair proposals received from AWEA.\")\n",
        "    else:\n",
        "        chosen_proposal = repair_proposals[0]\n",
        "        state[\"final_choice_ontology\"] = chosen_proposal[\"ontology\"]\n",
        "        state[\"final_choice_temp_file_path\"] = chosen_proposal.get(\"temp_file_path\")\n",
        "        state[\"log\"].append({\n",
        "            \"agent\": \"HITL2\", \"decision\": \"accepted_repair_proposal\",\n",
        "            \"justification\": chosen_proposal[\"justification\"],\n",
        "            \"chosen_ontology_name\": chosen_proposal[\"ontology\"].name\n",
        "        })\n",
        "        print(f\"HITL2: Accepted repair proposal. Chosen ontology: {chosen_proposal['ontology'].name}.\")\n",
        "    return state\n",
        "\n",
        "\n",
        "# 5.8. Meta-Knowledge Agent (MKA)\n",
        "def meta_knowledge_agent(state):\n",
        "    print(\"--- Running Meta-Knowledge Agent ---\")\n",
        "    ontology_to_log = state.get(\"final_choice_ontology\", state.get(\"ontology\"))\n",
        "    final_onto_axiom_strings = []\n",
        "\n",
        "    if ontology_to_log:\n",
        "        print(f\"MKA: Logging axioms for ontology: {ontology_to_log.name}\")\n",
        "        try:\n",
        "            for cls in ontology_to_log.classes():\n",
        "                for sup_axiom in cls.is_a:\n",
        "                    axiom_str = None\n",
        "                    if CLASS_CONSTRUCT_IMPORTED and SomeValuesFrom and AllValuesFrom and \\\n",
        "                       isinstance(sup_axiom, (SomeValuesFrom, AllValuesFrom)): # GUARDED USE\n",
        "                        restriction_type = \"some\" if isinstance(sup_axiom, SomeValuesFrom) else \"all\"\n",
        "                        prop_name_str = sup_axiom.property.name if hasattr(sup_axiom.property, \"name\") else str(sup_axiom.property)\n",
        "                        filler_name_str = \"None\"\n",
        "                        if hasattr(sup_axiom, \"filler\") and sup_axiom.filler is not None:\n",
        "                            filler_name_str = sup_axiom.filler.name if hasattr(sup_axiom.filler, \"name\") else str(sup_axiom.filler)\n",
        "                        axiom_str = f\"{cls.name} ⊑ {prop_name_str} {restriction_type} {filler_name_str}\"\n",
        "                    elif CLASS_CONSTRUCT_IMPORTED and MinCardinality and MaxCardinality and ExactCardinality and \\\n",
        "                         isinstance(sup_axiom, (MinCardinality, MaxCardinality, ExactCardinality)): # GUARDED USE\n",
        "                        r_type_str = \"min\"\n",
        "                        if isinstance(sup_axiom, MaxCardinality): r_type_str = \"max\"\n",
        "                        elif isinstance(sup_axiom, ExactCardinality): r_type_str = \"exactly\"\n",
        "                        prop_name_str = sup_axiom.property.name if hasattr(sup_axiom.property, \"name\") else str(sup_axiom.property)\n",
        "                        cardinality_str = str(sup_axiom.cardinality)\n",
        "                        filler_name_str = \"owl.Thing\"\n",
        "                        current_filler = getattr(sup_axiom, \"filler\", getattr(sup_axiom, \"_filler\", None))\n",
        "                        if isinstance(sup_axiom.property, DataProperty) and current_filler is None:\n",
        "                            filler_name_str = \"rdfs:Literal\"\n",
        "                        if current_filler is not None and current_filler is not Thing:\n",
        "                            if hasattr(current_filler, \"name\"):\n",
        "                                filler_name_str = current_filler.name\n",
        "                            else:\n",
        "                                type_map = {int: \"xsd:integer\", str: \"xsd:string\", float: \"xsd:double\", bool: \"xsd:boolean\"}\n",
        "                                filler_name_str = type_map.get(type(current_filler), str(current_filler))\n",
        "                        axiom_str = f\"{cls.name} ⊑ {prop_name_str} {r_type_str} {cardinality_str} {filler_name_str}\"\n",
        "                    elif hasattr(sup_axiom, \"name\"):\n",
        "                        axiom_str = f\"{cls.name} ⊑ {sup_axiom.name}\"\n",
        "                    else:\n",
        "                        axiom_str = f\"{cls.name} ⊑ {str(sup_axiom)}\"\n",
        "                    if axiom_str: final_onto_axiom_strings.append(axiom_str)\n",
        "\n",
        "            for dis_axiom_obj in ontology_to_log.disjoint_classes():\n",
        "                if hasattr(dis_axiom_obj, 'entities'):\n",
        "                    entity_names = [e.name for e in dis_axiom_obj.entities if hasattr(e, 'name')]\n",
        "                    if entity_names: final_onto_axiom_strings.append(f\"DisjointClasses({', '.join(entity_names)})\")\n",
        "        except AttributeError as attr_e:\n",
        "            print(f\"MKA: Warning - AttributeError during axiom serialization (likely due to missing restriction classes): {attr_e}\")\n",
        "            print(f\"MKA: CLASS_CONSTRUCT_IMPORTED is {CLASS_CONSTRUCT_IMPORTED}\")\n",
        "            final_onto_axiom_strings = [\"<serialization_error_mka_attribute_error>\"]\n",
        "        except Exception as e:\n",
        "            import traceback\n",
        "            print(f\"MKA: Warning - Failed to serialize axioms for logging from {ontology_to_log.name if ontology_to_log else 'None ontology'}: {e}\")\n",
        "            print(traceback.format_exc())\n",
        "            final_onto_axiom_strings = [\"<serialization_error_mka>\"]\n",
        "    else:\n",
        "        final_onto_axiom_strings = [\"<ontology_not_available_for_mka_logging>\"]\n",
        "\n",
        "    repair_just_text = state[\"repair_proposals\"][0].get(\"justification\") if state.get(\"repair_proposals\") else None\n",
        "    record = {\n",
        "        \"cycle_id\": state.get(\"cycle_id\"), \"candidate_axiom\": state.get(\"candidate_axiom\"),\n",
        "        \"dea_assessment\": state.get(\"dea_assessment\"), \"lia_assessment\": state.get(\"lia_assessment\"),\n",
        "        \"consistency_check_result\": state.get(\"consistent_with_candidate\"),\n",
        "        \"mis_list_if_inconsistent\": state.get(\"mis_list\"),\n",
        "        \"hitl1_chosen_strategy\": state.get(\"chosen_strategy\"),\n",
        "        \"repair_justification\": repair_just_text,\n",
        "        \"final_recorded_ontology_axioms\": final_onto_axiom_strings,\n",
        "        \"full_log_trace\": state.get(\"log\", [])\n",
        "    }\n",
        "    state[\"mka_record\"] = record\n",
        "    print(\"\\n===== MKA Log (JSON) =====\")\n",
        "    try:\n",
        "        print(json.dumps(record, indent=2, ensure_ascii=False))\n",
        "    except TypeError as e:\n",
        "        print(f\"MKA Error: Could not serialize MKA log to JSON: {e}\")\n",
        "        for k, v in record.items(): print(f\"  {k}: {str(v)[:500]}\")\n",
        "    print(\"===== End of MKA Log =====\\n\")\n",
        "\n",
        "    previous_ontology_temp_file = state.get(\"current_ontology_temp_file_path\")\n",
        "    new_main_ontology_temp_file = None # Will hold the path for the new state[\"ontology\"]\n",
        "\n",
        "    old_onto_ref = state.get(\"ontology\") # Keep a reference to the current main ontology\n",
        "\n",
        "    if state.get(\"final_choice_ontology\"):\n",
        "        state[\"ontology\"] = state[\"final_choice_ontology\"]\n",
        "        new_main_ontology_temp_file = state.get(\"final_choice_temp_file_path\")\n",
        "        state[\"current_ontology_temp_file_path\"] = new_main_ontology_temp_file\n",
        "        print(f\"MKA: Main ontology updated to repaired version: {state['ontology'].name}\")\n",
        "\n",
        "        # Close the world of the old_onto_ref if it's different from the new one and not default_world\n",
        "        if old_onto_ref and hasattr(old_onto_ref, '_world') and \\\n",
        "           state[\"ontology\"] and hasattr(state[\"ontology\"], '_world') and \\\n",
        "           old_onto_ref._world is not state[\"ontology\"]._world and \\\n",
        "           old_onto_ref._world is not owlready2.default_world:\n",
        "            if hasattr(old_onto_ref._world, 'closed') and not old_onto_ref._world.closed:\n",
        "                old_onto_ref._world.close()\n",
        "                print(f\"MKA: Closed world of superseded ontology {old_onto_ref.name}\")\n",
        "    else:\n",
        "        # No change to state[\"ontology\"], so its temp file path also remains.\n",
        "        new_main_ontology_temp_file = state.get(\"current_ontology_temp_file_path\")\n",
        "        if state.get(\"consistent_with_candidate\") and state.get(\"chosen_strategy\") == \"Accept Axiom\":\n",
        "            print(f\"MKA: Candidate axiom was consistent and 'accepted'. Main ontology '{state['ontology'].name}' unchanged by this MKA step.\")\n",
        "        else:\n",
        "            print(f\"MKA: No repair chosen or applied. Main ontology '{state['ontology'].name}' remains unchanged.\")\n",
        "\n",
        "\n",
        "    # Clean up the *previous* main ontology's temp file ONLY if it's different from the new one\n",
        "    if previous_ontology_temp_file and previous_ontology_temp_file != new_main_ontology_temp_file and os.path.exists(previous_ontology_temp_file):\n",
        "        try:\n",
        "            os.unlink(previous_ontology_temp_file)\n",
        "            print(f\"MKA: Cleaned up previous main ontology temp file: {previous_ontology_temp_file}\")\n",
        "        except Exception as e_unlink:\n",
        "            print(f\"MKA: Error cleaning up previous main ontology temp file {previous_ontology_temp_file}: {e_unlink}\")\n",
        "    return state\n",
        "\n",
        "\n",
        "# Cell 6: Build and Compile the LangGraph Workflow\n",
        "\n",
        "if 'onto' not in globals() or onto is None:\n",
        "    raise RuntimeError(\"Initial ontology 'onto' is not defined. Please ensure Cell 3 ran successfully.\")\n",
        "\n",
        "initial_state = {\n",
        "    \"messages\": [],\n",
        "    \"ontology\": onto,\n",
        "    \"current_ontology_temp_file_path\": temp_onto_file_path if 'temp_onto_file_path' in globals() and temp_onto_file_path else None,\n",
        "    \"cycle_id\": 1,\n",
        "    \"candidate_axiom\": None,\n",
        "    \"dea_assessment\": None,\n",
        "    \"lia_assessment\": None,\n",
        "    \"consistent_with_candidate\": True,\n",
        "    \"mis_list\": [],\n",
        "    \"chosen_strategy\": None,\n",
        "    \"repair_proposals\": [],\n",
        "    \"final_choice_ontology\": None,\n",
        "    \"final_choice_temp_file_path\": None,\n",
        "    \"log\": [],\n",
        "    \"mka_record\": None,\n",
        "}\n",
        "\n",
        "builder = StateGraph(type(initial_state))\n",
        "\n",
        "builder.add_node(\"LLMGen\", llm_generator)\n",
        "builder.add_node(\"DEA\", domain_expert_agent)\n",
        "builder.add_node(\"LIA\", linguistic_insight_agent)\n",
        "builder.add_node(\"CGA\", consistency_guard_agent)\n",
        "builder.add_node(\"HITL1\", hitl_stage1)\n",
        "builder.add_node(\"AWEA\", axiom_weakening_agent)\n",
        "builder.add_node(\"HITL2\", hitl_stage2)\n",
        "builder.add_node(\"MKA\", meta_knowledge_agent)\n",
        "\n",
        "builder.set_entry_point(\"LLMGen\")\n",
        "builder.add_edge(\"LLMGen\", \"DEA\")\n",
        "builder.add_edge(\"DEA\", \"LIA\")\n",
        "builder.add_edge(\"LIA\", \"CGA\")\n",
        "\n",
        "def decide_after_cga(state):\n",
        "    return \"TO_HITL1\"\n",
        "\n",
        "builder.add_conditional_edges(\"CGA\", decide_after_cga, {\"TO_HITL1\": \"HITL1\"})\n",
        "\n",
        "def decide_after_hitl1(state):\n",
        "    if state.get(\"chosen_strategy\") == \"Ontology Evolution\":\n",
        "        return \"TO_AWEA\"\n",
        "    else:\n",
        "        return \"TO_MKA\"\n",
        "\n",
        "builder.add_conditional_edges(\"HITL1\", decide_after_hitl1, {\"TO_AWEA\": \"AWEA\", \"TO_MKA\": \"MKA\"})\n",
        "\n",
        "builder.add_edge(\"AWEA\", \"HITL2\")\n",
        "builder.add_edge(\"HITL2\", \"MKA\")\n",
        "builder.add_edge(\"MKA\", END)\n",
        "\n",
        "workflow = builder.compile()\n",
        "print(\"LangGraph workflow compiled.\")\n",
        "\n",
        "\n",
        "# Cell 7: Run One ADORE Cycle\n",
        "final_result_state = None\n",
        "try:\n",
        "    print(\"\\n=== Starting ADORE Cycle ===\\n\")\n",
        "    # MODIFIED: Custom copy for initial_state to avoid deepcopying 'onto'\n",
        "    current_run_state = {}\n",
        "    for key, value in initial_state.items():\n",
        "        if key == \"ontology\": # Pass 'onto' by reference\n",
        "            current_run_state[key] = value\n",
        "        elif key == \"log\": # Shallow copy for lists that are appended to\n",
        "             current_run_state[key] = list(value) # Create a new list instance\n",
        "        elif key == \"repair_proposals\": # Shallow copy for lists that are appended to\n",
        "             current_run_state[key] = list(value)\n",
        "        else: # Deepcopy other potentially mutable items if necessary, or shallow if immutable\n",
        "            try:\n",
        "                current_run_state[key] = copy.deepcopy(value)\n",
        "            except TypeError: # Fallback for types that deepcopy might struggle with but are not ontology objects\n",
        "                current_run_state[key] = copy.copy(value)\n",
        "\n",
        "\n",
        "    final_result_state = workflow.invoke(current_run_state)\n",
        "    print(\"\\n=== ADORE Cycle Finished ===\\n\")\n",
        "\n",
        "    final_active_ontology = final_result_state.get(\"ontology\")\n",
        "    print(\"--- Final Active Ontology State after Cycle ---\")\n",
        "    if final_active_ontology:\n",
        "        print_axioms(final_active_ontology)\n",
        "        active_temp_file = final_result_state.get(\"current_ontology_temp_file_path\")\n",
        "        if active_temp_file and os.path.exists(active_temp_file):\n",
        "            print(f\"Note: Final active ontology is associated with temp file: {active_temp_file}. \")\n",
        "            print(\"Consider manual cleanup or uncomment os.unlink in script if it should be deleted.\")\n",
        "    else:\n",
        "        print(\"No final active ontology object found in the result state.\")\n",
        "\n",
        "except Exception as e_cycle:\n",
        "    import traceback\n",
        "    print(f\"!!! An error occurred during the ADORE cycle execution: {e_cycle} !!!\")\n",
        "    traceback.print_exc()\n",
        "finally:\n",
        "    print(\"\\n--- Final Script Cleanup ---\")\n",
        "    # Cleanup initial temp file if it wasn't passed on or handled by MKA\n",
        "    initial_temp_file_path_from_state = initial_state.get(\"current_ontology_temp_file_path\")\n",
        "    final_active_temp_file_path_from_state = final_result_state.get(\"current_ontology_temp_file_path\") if final_result_state else None\n",
        "\n",
        "    if initial_temp_file_path_from_state and \\\n",
        "       initial_temp_file_path_from_state != final_active_temp_file_path_from_state and \\\n",
        "       os.path.exists(initial_temp_file_path_from_state):\n",
        "        try:\n",
        "            os.unlink(initial_temp_file_path_from_state)\n",
        "            print(f\"Cleaned up initial state temp file: {initial_temp_file_path_from_state}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error cleaning up initial state temp file {initial_temp_file_path_from_state}: {e}\")\n",
        "\n",
        "    # The temp file for the final_active_ontology (if it exists and is managed by current_ontology_temp_file_path)\n",
        "    # should ideally be cleaned up if this is the absolute end of its lifecycle.\n",
        "    # For now, we leave it, as per the note in the try block.\n",
        "    # If you want to delete it:\n",
        "    # if final_active_temp_file_path_from_state and os.path.exists(final_active_temp_file_path_from_state):\n",
        "    #     try:\n",
        "    #         os.unlink(final_active_temp_file_path_from_state)\n",
        "    #         print(f\"Cleaned up final active ontology temp file: {final_active_temp_file_path_from_state}\")\n",
        "    #     except Exception as e:\n",
        "    #         print(f\"Error cleaning up final active temp file {final_active_temp_file_path_from_state}: {e}\")\n",
        "\n",
        "    print(\"Script execution finished.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JY_0IMG_AGWy",
        "outputId": "8c88db2f-74b7-4ce0-c8e1-010d6a093ea5"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Could not import class_construct from Owlready2. Axiom processing involving SomeValuesFrom, AllValuesFrom, etc., might fail.\n",
            "Initialized MockWatsonxLLM (secretly using OpenAI)\n",
            "Using watsonx_llm instance: True\n",
            "Initial ontology saved to temporary file: /tmp/tmpyuwpr5kg.owl\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "* Owlready2 * Running HermiT...\n",
            "    java -Xmx2000M -cp /usr/local/lib/python3.11/dist-packages/owlready2/hermit:/usr/local/lib/python3.11/dist-packages/owlready2/hermit/HermiT.jar org.semanticweb.HermiT.cli.CommandLine -c -O -D -I file:////tmp/tmp401ts6ln\n",
            "* Owlready2 * HermiT took 0.5991714000701904 seconds\n",
            "* Owlready * (NB: only changes on entities loaded in Python are shown, other changes are done but not listed)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial ontology created, saved, and checked for consistency.\n",
            "LangGraph workflow compiled.\n",
            "\n",
            "=== Starting ADORE Cycle ===\n",
            "\n",
            "--- Running LLM Generator Agent ---\n",
            "Send Prompt to AI: Given the ontology axioms:\n",
            "1) Pneumonia ⊑ ∃causedBy.Bacterium\n",
            "2) NovelVirusX ⊑ ¬Bacterium\n",
            "3) Pneumonia ⊑ ≤1 causedBy.⊤\n",
            "\n",
            "Please propose a single new DL axiom suggesting that Pneumonia can be caused by NovelVirusX, written in Description Logic syntax. Output exactly one line like:\n",
            "Pneumonia ⊑ ∃causedBy.NovelVirusX\n",
            "\n",
            "MockWatsonxLLM received messages (forwarding to OpenAI): [HumanMessage(content='Given the ontology axioms:\\n1) Pneumonia ⊑ ∃causedBy.Bacterium\\n2) NovelVirusX ⊑ ¬Bacterium\\n3) Pneumonia ⊑ ≤1 causedBy.⊤\\n\\nPlease propose a single new DL axiom suggesting that Pneumonia can be caused by NovelVirusX, written in Description Logic syntax. Output exactly one line like:\\nPneumonia ⊑ ∃causedBy.NovelVirusX\\n', additional_kwargs={}, response_metadata={})]\n",
            "AI proposed axiom: Pneumonia ⊑ ∃causedBy.NovelVirusX\n",
            "--- Running Domain Expert Agent ---\n",
            "--- Running Linguistic Insight Agent ---\n",
            "--- Running Consistency Guard Agent ---\n",
            "Duplicated ontology pneumonia_ontology into new world with IRI http://example.org/pneumonia_ontology.owl# (temp file: /tmp/tmp8gwuc9sh.owl)\n",
            "CGA: Successfully duplicated ontology. Original: pneumonia_ontology, Temp copy: tmp8gwuc9sh\n",
            "CGA: Warning - SomeValuesFrom class not imported, relying on property.some() internal behavior.\n",
            "CGA: Added to temp_onto_cga 'tmp8gwuc9sh': Pneumonia ⊑ causedBy some NovelVirusX\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "* Owlready2 * Running HermiT...\n",
            "    java -Xmx2000M -cp /usr/local/lib/python3.11/dist-packages/owlready2/hermit:/usr/local/lib/python3.11/dist-packages/owlready2/hermit/HermiT.jar org.semanticweb.HermiT.cli.CommandLine -c -O -D -I file:////tmp/tmpt_d94u0g\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CGA: Candidate axiom 'Pneumonia ⊑ ∃causedBy.NovelVirusX' is consistent with the (temporarily modified) ontology.\n",
            "--- Running HITL Stage 1 ---\n",
            "--- Running Meta-Knowledge Agent ---\n",
            "\n",
            "===== MKA Log (JSON) =====\n",
            "{\n",
            "  \"cycle_id\": 1,\n",
            "  \"candidate_axiom\": \"Pneumonia ⊑ ∃causedBy.NovelVirusX\",\n",
            "  \"dea_assessment\": {\n",
            "    \"score\": 0.9,\n",
            "    \"justification\": \"Contains NovelVirusX, which is medically plausible.\"\n",
            "  },\n",
            "  \"lia_assessment\": {\n",
            "    \"score\": 0.8,\n",
            "    \"justification\": \"Syntactically coherent DL axiom.\"\n",
            "  },\n",
            "  \"consistency_check_result\": true,\n",
            "  \"mis_list_if_inconsistent\": [],\n",
            "  \"hitl1_chosen_strategy\": \"Accept Axiom\",\n",
            "  \"repair_justification\": null,\n",
            "  \"final_recorded_ontology_axioms\": [\n",
            "    \"<ontology_not_available_for_mka_logging>\"\n",
            "  ],\n",
            "  \"full_log_trace\": [\n",
            "    {\n",
            "      \"agent\": \"LLMGen\",\n",
            "      \"action\": \"proposed_axiom\",\n",
            "      \"axiom\": \"Pneumonia ⊑ ∃causedBy.NovelVirusX\"\n",
            "    },\n",
            "    {\n",
            "      \"agent\": \"DEA\",\n",
            "      \"assessment\": {\n",
            "        \"score\": 0.9,\n",
            "        \"justification\": \"Contains NovelVirusX, which is medically plausible.\"\n",
            "      }\n",
            "    },\n",
            "    {\n",
            "      \"agent\": \"LIA\",\n",
            "      \"assessment\": {\n",
            "        \"score\": 0.8,\n",
            "        \"justification\": \"Syntactically coherent DL axiom.\"\n",
            "      }\n",
            "    },\n",
            "    {\n",
            "      \"agent\": \"CGA\",\n",
            "      \"status\": \"consistent\",\n",
            "      \"axiom_added_to_temp\": \"Pneumonia ⊑ ∃causedBy.NovelVirusX\"\n",
            "    },\n",
            "    {\n",
            "      \"agent\": \"HITL1\",\n",
            "      \"strategy\": \"Accept Axiom\",\n",
            "      \"rationale\": \"Candidate axiom is consistent. Simulating choice to accept (no direct addition in this step).\"\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "===== End of MKA Log =====\n",
            "\n",
            "MKA: Candidate axiom was consistent and 'accepted'. Main ontology 'pneumonia_ontology' unchanged by this MKA step.\n",
            "\n",
            "=== ADORE Cycle Finished ===\n",
            "\n",
            "--- Final Active Ontology State after Cycle ---\n",
            "\n",
            "=== Current Ontology Axioms (pneumonia_ontology) ===\n",
            "Pneumonia ⊑ owl.Thing\n",
            "Pneumonia ⊑ pneumonia_ontology.causedBy.some(pneumonia_ontology.Bacterium)\n",
            "Pneumonia ⊑ pneumonia_ontology.causedBy.max(1, owl.Thing)\n",
            "Pneumonia ⊑ pneumonia_ontology.causedBy.some(pneumonia_ontology.Bacterium)\n",
            "Pneumonia ⊑ pneumonia_ontology.causedBy.max(1, owl.Thing)\n",
            "Pneumonia ⊑ pneumonia_ontology.causedBy.some(pneumonia_ontology.Bacterium)\n",
            "Pneumonia ⊑ pneumonia_ontology.causedBy.max(1, owl.Thing)\n",
            "Pneumonia ⊑ pneumonia_ontology.causedBy.some(pneumonia_ontology.Bacterium)\n",
            "Pneumonia ⊑ pneumonia_ontology.causedBy.max(1, owl.Thing)\n",
            "Bacterium ⊑ owl.Thing\n",
            "NovelVirusX ⊑ owl.Thing\n",
            "Pathogen ⊑ owl.Thing\n",
            "=== End of Axioms ===\n",
            "\n",
            "Note: Final active ontology is associated with temp file: /tmp/tmpyuwpr5kg.owl. \n",
            "Consider manual cleanup or uncomment os.unlink in script if it should be deleted.\n",
            "\n",
            "--- Final Script Cleanup ---\n",
            "Script execution finished.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "* Owlready2 * HermiT took 0.5934405326843262 seconds\n",
            "* Owlready * (NB: only changes on entities loaded in Python are shown, other changes are done but not listed)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ADORE\n",
        "\n",
        "The Python code defines a workflow using LangGraph to simulate an ontology evolution process inspired by the ADORE (Axiom Discovery and Ontology Repair and Evolution) framework. The core idea is to propose new axioms for an ontology, assess them using different \"agents\" (simulated functions), check for consistency, and potentially repair the ontology if an inconsistency is found.\n",
        "\n",
        "Here's a breakdown of the workflow:\n",
        "\n",
        "1.  **Imports and Setup (Cell 1)**: Imports necessary libraries including `owlready2` for ontology manipulation, `langgraph` for defining the workflow, `langchain` components (though heavily mocked here), and standard Python modules like `os`, `json`, `tempfile`, and `uuid`. It also handles conditional imports and fetching API keys (though Watsonx is mocked, OpenAI keys are needed for the mock).\n",
        "2.  **Mocking WatsonxLLM (Cell 2)**: Defines a `MockWatsonxLLM` class that mimics the interface of a Watsonx LLM but internally uses a `ChatOpenAI` instance. This allows the code to run without actual Watsonx credentials, using OpenAI instead.\n",
        "3.  **Initial Ontology Creation (Cell 3)**: Uses `owlready2` to create a simple ontology (`pneumonia_ontology.owl`) in memory. It defines core classes (`Pneumonia`, `Bacterium`, `NovelVirusX`, `Pathogen`) and an object property (`causedBy`). It adds initial axioms (e.g., Pneumonia is caused by some Bacterium, Bacterium is disjoint from NovelVirusX, Pneumonia is caused by at most one Thing). The ontology is saved to a temporary file and a reasoner is run.\n",
        "4.  **Helper Functions (Cell 4)**:\n",
        "    *   `_duplicate_ontology`: Creates a deep copy of an `owlready2` ontology by saving it to a temporary file and loading it into a new `World`. This is crucial for consistency checks and repair operations to avoid modifying the original ontology state within the workflow.\n",
        "    *   `is_consistent`: Checks if an ontology is consistent by duplicating it, running the reasoner, and checking for inconsistent classes.\n",
        "    *   `print_axioms`: A utility to print the defined axioms in a more readable Description Logic-like format.\n",
        "5.  **Define Agents (Cell 5)**: This section defines Python functions that act as the \"agents\" in the ADORE workflow. Each function takes the current `state` dictionary as input, performs its task, updates the state, and returns the updated state.\n",
        "    *   `llm_generator`: Simulates an LLM proposing a new axiom based on the current ontology axioms. It asks the mocked LLM to propose an axiom suggesting Pneumonia can be caused by `NovelVirusX`.\n",
        "    *   `domain_expert_agent (DEA)`: Simulates a domain expert assessing the plausibility of the proposed axiom. It assigns a simple score and justification based on whether `NovelVirusX` is present in the axiom (a simplified check).\n",
        "    *   `linguistic_insight_agent (LIA)`: Simulates an agent checking the syntax or structure of the proposed axiom. It gives a score based on whether the string looks like an existential restriction axiom (`⊑ ∃` and `.`).\n",
        "    *   `consistency_guard_agent (CGA)`: The critical agent. It duplicates the *current* ontology, attempts to add the `candidate_axiom` to the duplicated ontology, and then runs the reasoner to check for inconsistencies. It updates the state with the consistency check result and, if inconsistent, a list of minimal inconsistent subsets (MIS, though this is hardcoded for the specific inconsistency this workflow is designed to expose).\n",
        "    *   `hitl_stage1`: Simulates a Human-in-the-Loop decision node. Based on whether the `CGA` found an inconsistency, it chooses a strategy: \"Ontology Evolution\" (meaning repair is needed) or \"Accept Axiom\" (meaning the axiom is consistent with the current ontology).\n",
        "    *   `axiom_weakening_agent (AWEA)`: Activated if the \"Ontology Evolution\" strategy is chosen. It attempts to repair the ontology. In this specific implementation, it duplicates the original ontology, removes the axiom `Pneumonia ⊑ ∃causedBy.Bacterium`, adds a new class `Pathogen` and axioms making `Bacterium` and `NovelVirusX` subclasses of `Pathogen`, adds `Pneumonia ⊑ ∃causedBy.Pathogen`, and finally adds the original inconsistent `candidate_axiom`. It then checks if this repaired ontology is consistent. If successful, it adds the repaired ontology (and its temp file path) to the `repair_proposals` list.\n",
        "    *   `hitl_stage2`: Simulates a second Human-in-the-Loop stage. If repair proposals exist, it selects the first one as the `final_choice_ontology`.\n",
        "    *   `meta_knowledge_agent (MKA)`: Logs the entire process and the state of the chosen final ontology (either the original if consistent/accepted, or the repaired one). It serializes the axioms of the final ontology and stores them in the state's `mka_record`. It also manages cleaning up the temporary file of the *previous* main ontology if a new one was chosen (the repaired one).\n",
        "6.  **Build and Compile LangGraph (Cell 6)**:\n",
        "    *   Defines the `initial_state` dictionary containing the necessary data for the workflow, including the initial ontology object and its temp file path.\n",
        "    *   Creates a `StateGraph` builder.\n",
        "    *   Adds each agent function as a node in the graph.\n",
        "    *   Sets the entry point (`LLMGen`).\n",
        "    *   Adds edges to define the sequence of execution (`LLMGen` -> `DEA` -> `LIA` -> `CGA`).\n",
        "    *   Uses `add_conditional_edges` after `CGA` and `HITL1` to implement branching logic based on the state (`CGA` always goes to `HITL1` in this setup, but `HITL1` branches to `AWEA` if repair is needed, or directly to `MKA` if the axiom is consistent).\n",
        "    *   Adds edges from `AWEA` to `HITL2` and from `HITL2` to `MKA`.\n",
        "    *   Sets `MKA` as the `END` point of the graph.\n",
        "    *   Compiles the builder into a executable `workflow` object.\n",
        "7.  **Run ADORE Cycle (Cell 7)**:\n",
        "    *   Sets up a `try...finally` block to ensure cleanup actions run.\n",
        "    *   Creates a copy of the `initial_state` to avoid modifying the original dictionary during the run (special handling for the ontology object and lists).\n",
        "    *   Invokes the compiled `workflow` with the starting state.\n",
        "    *   Prints a header before and after the cycle.\n",
        "    *   In the `finally` block, it attempts to clean up temporary files created during the workflow execution, particularly the initial ontology file if it was replaced by a repaired version.\n",
        "\n",
        "In essence, the code sets up an automated pipeline where a proposed ontology axiom is generated by an \"LLM\", assessed by \"experts\", checked for consistency, and if found inconsistent, a predefined \"repair\" strategy (axiom weakening) is applied, and the result is logged. The use of `owlready2` allows for actual ontology manipulation and reasoning within the simulation. The graph structure orchestrates the flow between these different functional components."
      ],
      "metadata": {
        "id": "iWzGYLQyAMcM"
      }
    }
  ]
}